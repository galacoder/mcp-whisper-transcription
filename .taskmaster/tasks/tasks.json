{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Set up Git Worktree",
        "description": "Create and configure git worktree for mcp-whisper-transcription development",
        "details": "Set up an isolated git worktree for developing the mcp-whisper-transcription server. This allows parallel development without affecting the main repository.",
        "testStrategy": "",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Set up Python project structure with FastMCP dependencies",
        "description": "Set up the Python project structure with FastMCP dependencies and existing whisper transcription code",
        "details": "Create the foundational Python project structure including:\n\n1. Create pyproject.toml with Poetry configuration:\n   - Project name: mcp-whisper-transcription\n   - Python version: ^3.9\n   - Dependencies:\n     * fastmcp>=2.0.0\n     * mlx-whisper>=0.4.2\n     * ffmpeg-python>=0.2.0\n     * tqdm>=4.66.0\n     * psutil>=5.9.0\n     * humanize>=4.8.0\n   - Dev dependencies: pytest, black, isort\n\n2. Copy existing implementation files:\n   - transcribe_mlx.py from /Users/sangle/Dev/action/projects/@ai/whisper-transcription\n   - whisper_utils.py from /Users/sangle/Dev/action/projects/@ai/whisper-transcription\n   - Preserve all existing functionality\n\n3. Create directory structure:\n   - src/\n     * __init__.py\n     * whisper_mcp_server.py (main MCP server file)\n   - tests/\n     * __init__.py\n     * test_transcription.py\n   - examples/\n     * example_audio.m4a\n     * usage_example.py\n\n4. Create configuration files:\n   - .env.example with:\n     * DEFAULT_MODEL=mlx-community/whisper-large-v3-mlx\n     * OUTPUT_FORMATS=txt,md,srt\n     * MAX_WORKERS=4\n     * TEMP_DIR=./temp\n   - .gitignore (Python-specific)\n   - README.md (initial structure)\n\n5. Initialize Poetry environment:\n   - Run poetry init and configure\n   - Run poetry install to create lock file\n   - Verify all dependencies are installed",
        "testStrategy": "- Verify Poetry environment activates correctly\n- Test import of all dependencies\n- Ensure existing transcribe_mlx.py and whisper_utils.py work in new environment\n- Run a simple transcription test to verify MLX Whisper loads",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 3,
        "title": "Create FastMCP server wrapper with initialization",
        "description": "Create the FastMCP server wrapper that integrates with existing WhisperTranscriber implementation",
        "details": "Implement the main MCP server structure in src/whisper_mcp_server.py:\n\n1. FastMCP Server Setup:\n   ```python\n   from fastmcp import FastMCP\n   import os\n   from pathlib import Path\n   from dotenv import load_dotenv\n   \n   # Load environment variables\n   load_dotenv()\n   \n   # Initialize FastMCP with metadata\n   mcp = FastMCP(\n       name=\"Whisper Transcription MCP\",\n       instructions=\"\"\"\n       This MCP server provides audio/video transcription using MLX-optimized Whisper models.\n       Optimized for Apple Silicon devices with ultra-fast performance.\n       \n       Available tools:\n       - transcribe_file: Transcribe a single file\n       - batch_transcribe: Process multiple files\n       - list_models: Show available Whisper models\n       \n       Supports multiple output formats: txt, md, srt, json\n       \"\"\"\n   )\n   ```\n\n2. Import and Initialize Components:\n   ```python\n   import sys\n   sys.path.append(str(Path(__file__).parent.parent))\n   \n   from transcribe_mlx import WhisperTranscriber\n   from whisper_utils import (\n       TranscriptionStats,\n       PerformanceReport,\n       OutputFormatter,\n       setup_logger\n   )\n   \n   # Global instances\n   logger = setup_logger(Path(\"logs\"), \"WhisperMCP\")\n   transcriber = None  # Lazy initialization\n   performance_report = PerformanceReport()\n   ```\n\n3. Configuration Management:\n   ```python\n   # Configuration from environment\n   DEFAULT_MODEL = os.getenv(\"DEFAULT_MODEL\", \"mlx-community/whisper-large-v3-mlx\")\n   DEFAULT_FORMATS = os.getenv(\"OUTPUT_FORMATS\", \"txt,md,srt\")\n   MAX_WORKERS = int(os.getenv(\"MAX_WORKERS\", \"4\"))\n   TEMP_DIR = Path(os.getenv(\"TEMP_DIR\", \"./temp\"))\n   \n   # Ensure directories exist\n   TEMP_DIR.mkdir(exist_ok=True)\n   Path(\"logs\").mkdir(exist_ok=True)\n   ```\n\n4. Lazy Transcriber Initialization:\n   ```python\n   def get_transcriber(model_name: str = None) -> WhisperTranscriber:\n       global transcriber\n       model = model_name or DEFAULT_MODEL\n       \n       if transcriber is None or transcriber.model_name != model:\n           logger.info(f\"Initializing transcriber with model: {model}\")\n           transcriber = WhisperTranscriber(\n               model_name=model,\n               output_formats=DEFAULT_FORMATS\n           )\n       return transcriber\n   ```\n\n5. Error Handling Setup:\n   ```python\n   class TranscriptionError(Exception):\n       \"\"\"Custom exception for transcription errors\"\"\"\n       pass\n   \n   # Add error handler middleware\n   @mcp.error_handler\n   async def handle_errors(error: Exception) -> dict:\n       logger.error(f\"MCP Error: {str(error)}\", exc_info=True)\n       return {\n           \"error\": True,\n           \"message\": str(error),\n           \"type\": error.__class__.__name__\n       }\n   ```\n\n6. Main Entry Point:\n   ```python\n   if __name__ == \"__main__\":\n       # Check dependencies\n       try:\n           import mlx\n           import ffmpeg\n       except ImportError as e:\n           logger.error(f\"Missing dependency: {e}\")\n           print(f\"Error: {e}\")\n           print(\"Please install all dependencies: poetry install\")\n           sys.exit(1)\n       \n       # Run the server\n       logger.info(\"Starting Whisper Transcription MCP Server\")\n       mcp.run()\n   ```",
        "testStrategy": "- Test server starts without errors\n- Verify FastMCP initialization with proper metadata\n- Test lazy loading of WhisperTranscriber\n- Verify environment variable loading\n- Test error handling for missing dependencies\n- Check logging is working correctly",
        "status": "done",
        "dependencies": [
          2
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 4,
        "title": "Integrate existing WhisperTranscriber with FastMCP context",
        "description": "Integrate the existing Python WhisperTranscriber implementation with FastMCP server",
        "details": "Configure the existing WhisperTranscriber class to work within the FastMCP context:\n\n1. No porting needed - use existing Python implementation as-is\n   - The transcribe_mlx.py file already contains a complete WhisperTranscriber class\n   - The whisper_utils.py contains all supporting utilities\n   - Both files will be copied to the project and imported directly\n\n2. Create singleton instance management:\n   ```python\n   # In whisper_mcp_server.py\n   transcriber_instance = None\n   \n   def get_transcriber(model_name: str = None) -> WhisperTranscriber:\n       global transcriber_instance\n       model = model_name or DEFAULT_MODEL\n       \n       # Only recreate if model changes\n       if transcriber_instance is None or transcriber_instance.model_name != model:\n           transcriber_instance = WhisperTranscriber(\n               model_name=model,\n               output_formats=DEFAULT_FORMATS\n           )\n       return transcriber_instance\n   ```\n\n3. Add configuration via environment variables:\n   - DEFAULT_MODEL: Default MLX Whisper model to use\n   - OUTPUT_FORMATS: Comma-separated list of output formats\n   - TEMP_DIR: Directory for temporary audio files\n   - MAX_WORKERS: Number of parallel workers for batch processing\n\n4. Ensure proper resource cleanup:\n   ```python\n   @mcp.on_shutdown\n   async def cleanup():\n       \"\"\"Clean up resources on server shutdown\"\"\"\n       if transcriber_instance:\n           # Clean up any temporary files\n           cleanup_temp_files(TEMP_DIR)\n       logger.info(\"Server shutdown complete\")\n   ```\n\n5. Model caching strategy:\n   - MLX models are cached by default in ~/.cache/huggingface/\n   - No additional caching needed - MLX handles this automatically\n   - Model switching will automatically download new models if needed",
        "testStrategy": "- Test that existing WhisperTranscriber class works unchanged\n- Verify singleton pattern prevents multiple model loads\n- Test model switching creates new instance\n- Verify temporary file cleanup on shutdown\n- Test all output formats work correctly",
        "status": "done",
        "dependencies": [
          3
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 5,
        "title": "Implement transcribe_file Tool",
        "description": "Create the main transcribe_file tool for single file transcription using FastMCP decorators",
        "details": "Implement the transcribe_file MCP tool with detailed parameters:\n\n```python\n@mcp.tool\nasync def transcribe_file(\n    file_path: str,\n    model: str = None,\n    output_formats: str = None,\n    language: str = \"en\",\n    task: str = \"transcribe\",\n    output_dir: str = None,\n    temperature: float = 0.0,\n    no_speech_threshold: float = 0.45,\n    initial_prompt: str = None\n) -> dict:\n    \"\"\"\n    Transcribe a single audio/video file using MLX Whisper.\n    \n    Args:\n        file_path: Path to audio/video file (required)\n        model: Whisper model to use (default: from environment)\n        output_formats: Comma-separated formats (txt,md,srt,json)\n        language: Language code (default: en)\n        task: Task type - 'transcribe' or 'translate'\n        output_dir: Directory for output files (default: same as input)\n        temperature: Sampling temperature (0.0 = deterministic)\n        no_speech_threshold: Silence detection threshold\n        initial_prompt: Optional prompt to guide transcription style\n    \n    Returns:\n        dict with:\n        - text: Full transcription text\n        - segments: List of timestamped segments\n        - output_files: Paths to generated files\n        - duration: Audio duration in seconds\n        - processing_time: Time taken to transcribe\n        - model_used: Name of the model used\n    \"\"\"\n    try:\n        # Validate file exists\n        input_path = Path(file_path)\n        if not input_path.exists():\n            raise TranscriptionError(f\"File not found: {file_path}\")\n        \n        # Get transcriber instance\n        transcriber = get_transcriber(model)\n        \n        # Set output directory\n        output_path = Path(output_dir) if output_dir else input_path.parent\n        output_base = output_path / input_path.stem\n        \n        # Override output formats if specified\n        if output_formats:\n            transcriber.output_formats = set(output_formats.split(','))\n        \n        # Extract audio if needed (video file)\n        if input_path.suffix.lower() in ['.mp4', '.mov', '.avi', '.mkv']:\n            audio_path = extract_audio(input_path, TEMP_DIR)\n        else:\n            audio_path = input_path\n        \n        # Perform transcription\n        start_time = time.time()\n        result = transcriber.transcribe_audio(\n            audio_path,\n            output_base,\n            language=language,\n            task=task,\n            temperature=temperature,\n            no_speech_threshold=no_speech_threshold,\n            initial_prompt=initial_prompt\n        )\n        processing_time = time.time() - start_time\n        \n        # Get list of output files\n        output_files = []\n        for fmt in transcriber.output_formats:\n            file_path = f\"{output_base}.{fmt}\"\n            if Path(file_path).exists():\n                output_files.append(file_path)\n        \n        # Clean up temp audio if extracted\n        if audio_path != input_path:\n            audio_path.unlink()\n        \n        return {\n            \"text\": result.get(\"text\", \"\"),\n            \"segments\": result.get(\"segments\", []),\n            \"output_files\": output_files,\n            \"duration\": get_video_duration(input_path),\n            \"processing_time\": processing_time,\n            \"model_used\": transcriber.model_name\n        }\n        \n    except Exception as e:\n        logger.error(f\"Transcription failed: {str(e)}\", exc_info=True)\n        raise TranscriptionError(f\"Transcription failed: {str(e)}\")\n```",
        "testStrategy": "- Test with various audio formats (mp3, wav, m4a)\n- Test with video files (mp4, mov)\n- Test invalid file paths\n- Test all output format combinations\n- Test language and task parameters\n- Verify proper error messages\n- Test concurrent transcriptions",
        "status": "done",
        "dependencies": [
          4
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 6,
        "title": "Implement batch_transcribe Tool",
        "description": "Create the batch_transcribe tool for processing multiple files with parallel processing",
        "details": "Implement the batch_transcribe MCP tool with advanced features:\n\n```python\n@mcp.tool\nasync def batch_transcribe(\n    directory: str,\n    pattern: str = \"*\",\n    recursive: bool = False,\n    max_workers: int = None,\n    output_formats: str = None,\n    skip_existing: bool = True,\n    output_dir: str = None\n) -> dict:\n    \"\"\"\n    Batch transcribe multiple files matching pattern.\n    \n    Args:\n        directory: Directory containing media files\n        pattern: Glob pattern for files (e.g., \"*.mp4\", \"audio_*.m4a\")\n        recursive: Search subdirectories recursively\n        max_workers: Number of parallel workers (default: from env)\n        output_formats: Output formats for all files\n        skip_existing: Skip files that already have transcripts\n        output_dir: Optional separate output directory\n    \n    Returns:\n        dict with:\n        - total_files: Number of files found\n        - processed: Number of files processed\n        - skipped: Number of files skipped\n        - failed: Number of files that failed\n        - results: Array of individual file results\n        - performance_report: Overall performance statistics\n    \"\"\"\n    try:\n        # Find matching files\n        dir_path = Path(directory)\n        if not dir_path.exists():\n            raise TranscriptionError(f\"Directory not found: {directory}\")\n        \n        if recursive:\n            files = list(dir_path.rglob(pattern))\n        else:\n            files = list(dir_path.glob(pattern))\n        \n        # Filter to supported formats\n        supported_extensions = {'.mp3', '.wav', '.m4a', '.mp4', '.mov', '.avi', '.mkv'}\n        media_files = [f for f in files if f.suffix.lower() in supported_extensions]\n        \n        if not media_files:\n            return {\n                \"total_files\": 0,\n                \"processed\": 0,\n                \"skipped\": 0,\n                \"failed\": 0,\n                \"results\": [],\n                \"performance_report\": \"No media files found\"\n            }\n        \n        # Check which files need processing\n        files_to_process = []\n        skipped_files = []\n        \n        for file in media_files:\n            output_base = Path(output_dir) / file.stem if output_dir else file.parent / file.stem\n            # Check if any output format already exists\n            needs_processing = True\n            if skip_existing:\n                for fmt in (output_formats or DEFAULT_FORMATS).split(','):\n                    if Path(f\"{output_base}.{fmt}\").exists():\n                        needs_processing = False\n                        break\n            \n            if needs_processing:\n                files_to_process.append(file)\n            else:\n                skipped_files.append(file)\n        \n        # Process files in parallel\n        results = []\n        failed = []\n        workers = max_workers or MAX_WORKERS\n        \n        # Create progress tracking\n        logger.info(f\"Processing {len(files_to_process)} files with {workers} workers\")\n        \n        # Process files using asyncio gather for true async\n        tasks = []\n        for file in files_to_process:\n            task = transcribe_file(\n                file_path=str(file),\n                output_formats=output_formats,\n                output_dir=output_dir\n            )\n            tasks.append(task)\n        \n        # Limit concurrent tasks\n        import asyncio\n        semaphore = asyncio.Semaphore(workers)\n        \n        async def process_with_semaphore(task):\n            async with semaphore:\n                try:\n                    return await task\n                except Exception as e:\n                    return {\"error\": str(e)}\n        \n        # Process all tasks\n        task_results = await asyncio.gather(\n            *[process_with_semaphore(task) for task in tasks]\n        )\n        \n        # Collect results\n        for file, result in zip(files_to_process, task_results):\n            if \"error\" in result:\n                failed.append({\"file\": str(file), \"error\": result[\"error\"]})\n            else:\n                results.append({\"file\": str(file), **result})\n        \n        # Generate performance report\n        total_duration = sum(r.get(\"duration\", 0) for r in results)\n        total_processing = sum(r.get(\"processing_time\", 0) for r in results)\n        \n        performance_summary = {\n            \"total_audio_duration\": total_duration,\n            \"total_processing_time\": total_processing,\n            \"average_speed\": total_duration / total_processing if total_processing > 0 else 0,\n            \"files_per_minute\": len(results) / (total_processing / 60) if total_processing > 0 else 0\n        }\n        \n        return {\n            \"total_files\": len(media_files),\n            \"processed\": len(results),\n            \"skipped\": len(skipped_files),\n            \"failed\": len(failed),\n            \"results\": results,\n            \"failed_files\": failed,\n            \"performance_report\": performance_summary\n        }\n        \n    except Exception as e:\n        logger.error(f\"Batch transcription failed: {str(e)}\", exc_info=True)\n        raise TranscriptionError(f\"Batch transcription failed: {str(e)}\")\n```",
        "testStrategy": "- Test with directory containing mixed media files\n- Test pattern matching (*.mp4, audio_*.m4a)\n- Test recursive directory search\n- Test skip_existing functionality\n- Test parallel processing with different worker counts\n- Test error handling for failed files\n- Verify performance metrics are accurate",
        "status": "done",
        "dependencies": [
          5
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 7,
        "title": "Create Support Tools",
        "description": "Implement supporting tools for model management and transcription utilities",
        "details": "Create additional MCP tools for enhanced functionality:\n\n1. **list_models** - List available MLX Whisper models:\n```python\n@mcp.tool\ndef list_models() -> dict:\n    \"\"\"List all available MLX Whisper models with details.\"\"\"\n    models = [\n        {\"id\": \"mlx-community/whisper-tiny-mlx\", \"size\": \"39M\", \"speed\": \"~10x\", \"accuracy\": \"Good\"},\n        {\"id\": \"mlx-community/whisper-base-mlx\", \"size\": \"74M\", \"speed\": \"~7x\", \"accuracy\": \"Better\"},\n        {\"id\": \"mlx-community/whisper-small-mlx\", \"size\": \"244M\", \"speed\": \"~5x\", \"accuracy\": \"Very Good\"},\n        {\"id\": \"mlx-community/whisper-medium-mlx\", \"size\": \"769M\", \"speed\": \"~3x\", \"accuracy\": \"Excellent\"},\n        {\"id\": \"mlx-community/whisper-large-v3-mlx\", \"size\": \"1550M\", \"speed\": \"~2x\", \"accuracy\": \"Best\"},\n        {\"id\": \"mlx-community/whisper-large-v3-turbo\", \"size\": \"809M\", \"speed\": \"~4x\", \"accuracy\": \"Excellent\"}\n    ]\n    return {\n        \"models\": models,\n        \"current_model\": transcriber.model_name if transcriber else DEFAULT_MODEL,\n        \"cache_dir\": str(Path.home() / \".cache\" / \"huggingface\")\n    }\n```\n\n2. **get_model_info** - Get specific model details:\n```python\n@mcp.tool\ndef get_model_info(model_id: str) -> dict:\n    \"\"\"Get detailed information about a specific Whisper model.\"\"\"\n    # Implementation returns model stats, requirements, etc.\n```\n\n3. **clear_cache** - Clear model cache:\n```python\n@mcp.tool\ndef clear_cache(model_id: str = None) -> dict:\n    \"\"\"Clear downloaded model cache.\n    \n    Args:\n        model_id: Specific model to clear, or None for all models\n    \"\"\"\n    # Clear from ~/.cache/huggingface/\n```\n\n4. **estimate_processing_time** - Estimate transcription time:\n```python\n@mcp.tool\ndef estimate_processing_time(\n    file_path: str,\n    model: str = None\n) -> dict:\n    \"\"\"Estimate processing time for a file.\n    \n    Returns:\n        dict with duration, estimated_time, and model_speed\n    \"\"\"\n    # Calculate based on file duration and model speed\n```\n\n5. **validate_media_file** - Check file compatibility:\n```python\n@mcp.tool\ndef validate_media_file(file_path: str) -> dict:\n    \"\"\"Validate if a file can be transcribed.\n    \n    Returns:\n        dict with is_valid, format, duration, issues\n    \"\"\"\n    # Check format, codec, duration, etc.\n```\n\n6. **get_supported_formats** - List all supported formats:\n```python\n@mcp.tool\ndef get_supported_formats() -> dict:\n    \"\"\"Get lists of supported input and output formats.\"\"\"\n    return {\n        \"input_formats\": {\n            \"audio\": [\".mp3\", \".wav\", \".m4a\", \".flac\", \".ogg\"],\n            \"video\": [\".mp4\", \".mov\", \".avi\", \".mkv\", \".webm\"]\n        },\n        \"output_formats\": {\n            \"txt\": \"Plain text with timestamps\",\n            \"md\": \"Markdown formatted text\",\n            \"srt\": \"SubRip subtitle format\",\n            \"json\": \"Full transcription data with segments\"\n        }\n    }\n```",
        "testStrategy": "- Test each tool independently\n- Verify model listing accuracy\n- Test cache clearing functionality\n- Validate time estimation accuracy\n- Test file validation with various formats\n- Ensure all tools handle errors gracefully",
        "status": "done",
        "dependencies": [
          5
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 8,
        "title": "Implement Resource Endpoints",
        "description": "Create MCP resource endpoints for transcription data access",
        "details": "Implement MCP resource endpoints for accessing transcription data:\n\n1. **transcription://history** - Recent transcriptions list:\n```python\n@mcp.resource(\"transcription://history\")\nasync def get_transcription_history() -> dict:\n    \"\"\"Get list of recent transcriptions.\"\"\"\n    # Read from a history.json file maintained by the server\n    history_file = Path(\"logs\") / \"transcription_history.json\"\n    \n    if history_file.exists():\n        with open(history_file) as f:\n            history = json.load(f)\n    else:\n        history = []\n    \n    # Return last 50 transcriptions\n    return {\n        \"transcriptions\": history[-50:],\n        \"total_count\": len(history)\n    }\n```\n\n2. **transcription://history/{id}** - Specific transcription details:\n```python\n@mcp.resource(\"transcription://history/{transcription_id}\")\nasync def get_transcription_details(transcription_id: str) -> dict:\n    \"\"\"Get detailed information about a specific transcription.\"\"\"\n    # Retrieve from history by ID\n    # Include full segments, metadata, performance stats\n```\n\n3. **transcription://models** - Available models resource:\n```python\n@mcp.resource(\"transcription://models\")\ndef get_models_resource() -> dict:\n    \"\"\"Resource endpoint for available models.\"\"\"\n    return list_models()  # Reuse the tool function\n```\n\n4. **transcription://config** - Current configuration:\n```python\n@mcp.resource(\"transcription://config\")\ndef get_config_resource() -> dict:\n    \"\"\"Get current server configuration.\"\"\"\n    return {\n        \"default_model\": DEFAULT_MODEL,\n        \"output_formats\": DEFAULT_FORMATS,\n        \"max_workers\": MAX_WORKERS,\n        \"temp_dir\": str(TEMP_DIR),\n        \"version\": \"1.0.0\"\n    }\n```\n\n5. **transcription://formats** - Supported formats:\n```python\n@mcp.resource(\"transcription://formats\")\ndef get_formats_resource() -> dict:\n    \"\"\"Resource for supported formats.\"\"\"\n    return get_supported_formats()  # Reuse the tool\n```\n\n6. **transcription://performance** - Performance statistics:\n```python\n@mcp.resource(\"transcription://performance\")\nasync def get_performance_stats() -> dict:\n    \"\"\"Get server performance statistics.\"\"\"\n    # Aggregate performance data\n    return {\n        \"total_transcriptions\": performance_report.total_files,\n        \"total_audio_hours\": performance_report.total_duration / 3600,\n        \"average_speed\": performance_report.average_speed,\n        \"uptime\": time.time() - server_start_time\n    }\n```\n\nHistory tracking implementation:\n```python\n# Add to transcribe_file after successful transcription\ndef record_transcription(file_path: str, result: dict):\n    \"\"\"Record transcription in history.\"\"\"\n    history_file = Path(\"logs\") / \"transcription_history.json\"\n    \n    entry = {\n        \"id\": str(uuid.uuid4()),\n        \"timestamp\": datetime.now().isoformat(),\n        \"file_path\": file_path,\n        \"model\": result[\"model_used\"],\n        \"duration\": result[\"duration\"],\n        \"processing_time\": result[\"processing_time\"],\n        \"output_files\": result[\"output_files\"]\n    }\n    \n    # Load existing history\n    if history_file.exists():\n        with open(history_file) as f:\n            history = json.load(f)\n    else:\n        history = []\n    \n    # Append and save\n    history.append(entry)\n    with open(history_file, 'w') as f:\n        json.dump(history, f, indent=2)\n```",
        "testStrategy": "- Test resource URI parsing and routing\n- Verify history is properly recorded\n- Test pagination for large histories\n- Test resource filtering parameters\n- Verify all resources return valid JSON\n- Test concurrent resource access",
        "status": "done",
        "dependencies": [
          5,
          6
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 9,
        "title": "Test MCP Integrations",
        "description": "Create comprehensive tests for all MCP tools and resources using pytest",
        "details": "Implement comprehensive testing suite:\n\n1. **Unit Tests** (tests/test_transcriber.py):\n```python\nimport pytest\nfrom pathlib import Path\nfrom src.whisper_mcp_server import get_transcriber\n\nclass TestWhisperTranscriber:\n    def test_singleton_pattern(self):\n        \"\"\"Test transcriber singleton behavior.\"\"\"\n        t1 = get_transcriber()\n        t2 = get_transcriber()\n        assert t1 is t2\n    \n    def test_model_switching(self):\n        \"\"\"Test changing models creates new instance.\"\"\"\n        t1 = get_transcriber(\"mlx-community/whisper-tiny-mlx\")\n        t2 = get_transcriber(\"mlx-community/whisper-base-mlx\")\n        assert t1 is not t2\n```\n\n2. **Integration Tests** (tests/test_mcp_tools.py):\n```python\nimport pytest\nfrom fastmcp import Client\nfrom src.whisper_mcp_server import mcp\n\n@pytest.fixture\nasync def client():\n    \"\"\"Create test client.\"\"\"\n    async with Client(mcp) as client:\n        yield client\n\nclass TestMCPTools:\n    async def test_transcribe_file(self, client):\n        \"\"\"Test single file transcription.\"\"\"\n        result = await client.call_tool(\n            \"transcribe_file\",\n            {\"file_path\": \"examples/test_audio.m4a\"}\n        )\n        assert \"text\" in result\n        assert \"segments\" in result\n    \n    async def test_batch_transcribe(self, client):\n        \"\"\"Test batch processing.\"\"\"\n        result = await client.call_tool(\n            \"batch_transcribe\",\n            {\n                \"directory\": \"examples\",\n                \"pattern\": \"*.m4a\"\n            }\n        )\n        assert result[\"processed\"] > 0\n```\n\n3. **Resource Tests** (tests/test_resources.py):\n```python\nclass TestMCPResources:\n    async def test_history_resource(self, client):\n        \"\"\"Test history resource endpoint.\"\"\"\n        resources = await client.read_resource(\"transcription://history\")\n        assert isinstance(resources, dict)\n        assert \"transcriptions\" in resources\n```\n\n4. **Performance Tests** (tests/test_performance.py):\n```python\nimport time\n\nclass TestPerformance:\n    async def test_transcription_speed(self, client):\n        \"\"\"Test transcription meets speed targets.\"\"\"\n        start = time.time()\n        result = await client.call_tool(\n            \"transcribe_file\",\n            {\n                \"file_path\": \"examples/1_minute_audio.m4a\",\n                \"model\": \"mlx-community/whisper-tiny-mlx\"\n            }\n        )\n        elapsed = time.time() - start\n        \n        # Should be faster than realtime\n        assert elapsed < 60  # Less than audio duration\n```\n\n5. **Error Handling Tests** (tests/test_errors.py):\n```python\nclass TestErrorHandling:\n    async def test_invalid_file(self, client):\n        \"\"\"Test handling of invalid files.\"\"\"\n        with pytest.raises(Exception) as exc:\n            await client.call_tool(\n                \"transcribe_file\",\n                {\"file_path\": \"nonexistent.mp3\"}\n            )\n        assert \"not found\" in str(exc.value)\n```\n\n6. **Mock Tests** (tests/test_mocks.py):\n```python\nfrom unittest.mock import Mock, patch\n\nclass TestWithMocks:\n    @patch('mlx_whisper.transcribe')\n    async def test_transcribe_mock(self, mock_transcribe, client):\n        \"\"\"Test with mocked MLX transcribe.\"\"\"\n        mock_transcribe.return_value = {\n            \"text\": \"Mocked transcription\",\n            \"segments\": []\n        }\n        \n        result = await client.call_tool(\n            \"transcribe_file\",\n            {\"file_path\": \"test.mp3\"}\n        )\n        assert result[\"text\"] == \"Mocked transcription\"\n```\n\n7. **Example Scripts** (examples/):\n- basic_transcription.py - Simple single file example\n- batch_processing.py - Batch transcription example\n- model_comparison.py - Compare different models\n- real_time_monitoring.py - Monitor transcription progress",
        "testStrategy": "- Run tests with pytest\n- Use pytest-asyncio for async tests\n- Mock MLX models for fast testing\n- Test with real audio files in integration tests\n- Measure code coverage (target 85%+)\n- Run performance benchmarks\n- Test on different audio formats and durations",
        "status": "in-progress",
        "dependencies": [
          8
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 10,
        "title": "Create Comprehensive Documentation",
        "description": "Write complete documentation for the MCP server",
        "details": "Create comprehensive documentation structure:\n\n1. **README.md** - Main documentation:\n```markdown\n# Whisper Transcription MCP Server\n\nUltra-fast audio/video transcription using MLX-optimized Whisper models for Apple Silicon.\n\n## Features\n- 🚀 Blazing fast transcription on Apple Silicon\n- 📁 Batch processing with parallel execution\n- 🎯 Multiple output formats (TXT, MD, SRT, JSON)\n- 🔧 FastMCP integration for easy tool access\n- 📊 Performance tracking and reporting\n\n## Quick Start\n\n### Installation\n```bash\n# Clone the repository\ngit clone https://github.com/yourusername/mcp-whisper-transcription\ncd mcp-whisper-transcription\n\n# Install dependencies\npoetry install\n\n# Run the server\npoetry run python src/whisper_mcp_server.py\n```\n\n### Claude Desktop Configuration\nAdd to your Claude Desktop config:\n```json\n{\n  \"mcpServers\": {\n    \"whisper\": {\n      \"command\": \"poetry\",\n      \"args\": [\"run\", \"python\", \"/path/to/src/whisper_mcp_server.py\"]\n    }\n  }\n}\n```\n```\n\n2. **SETUP.md** - Detailed setup guide:\n- System requirements (macOS, Apple Silicon)\n- FFmpeg installation\n- Poetry setup\n- Environment configuration\n- Troubleshooting common issues\n\n3. **API.md** - Complete API reference:\n- All tools with parameters and examples\n- All resources with response formats\n- Error codes and handling\n- Rate limits and performance considerations\n\n4. **MODELS.md** - Model comparison guide:\n- Model sizes and performance\n- Accuracy vs speed tradeoffs\n- Memory requirements\n- Use case recommendations\n\n5. **TROUBLESHOOTING.md** - Common issues:\n- FFmpeg not found\n- Model download failures\n- Memory issues with large files\n- Performance optimization tips\n\n6. **EXAMPLES.md** - Usage examples:\n```python\n# Single file transcription\nresult = await client.call_tool(\n    \"transcribe_file\",\n    {\n        \"file_path\": \"interview.mp4\",\n        \"output_formats\": \"txt,srt\",\n        \"model\": \"mlx-community/whisper-large-v3-turbo\"\n    }\n)\n\n# Batch processing\nresult = await client.call_tool(\n    \"batch_transcribe\",\n    {\n        \"directory\": \"./podcasts\",\n        \"pattern\": \"*.mp3\",\n        \"max_workers\": 4\n    }\n)\n```\n\n7. **CONTRIBUTING.md** - Contribution guidelines:\n- Code style (Black, isort)\n- Testing requirements\n- PR process\n- Issue templates\n\n8. **CHANGELOG.md** - Version history:\n- Version 1.0.0 - Initial release\n  - FastMCP integration\n  - MLX Whisper support\n  - Batch processing\n  - Multiple output formats",
        "testStrategy": "- Review all documentation for accuracy\n- Test all code examples\n- Verify setup instructions work on clean system\n- Check API documentation completeness\n- Ensure troubleshooting covers real issues\n- Get feedback from test users",
        "status": "in-progress",
        "dependencies": [
          9
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 11,
        "title": "Prepare for Release",
        "description": "Final preparation steps for releasing the MCP server",
        "details": "Prepare for public release:\n\n1. **Update pyproject.toml** with proper metadata:\n```toml\n[tool.poetry]\nname = \"mcp-whisper-transcription\"\nversion = \"1.0.0\"\ndescription = \"FastMCP server for audio/video transcription using MLX Whisper\"\nauthors = [\"Your Name <email@example.com>\"]\nlicense = \"MIT\"\nreadme = \"README.md\"\nrepository = \"https://github.com/yourusername/mcp-whisper-transcription\"\nkeywords = [\"mcp\", \"whisper\", \"transcription\", \"mlx\", \"fastmcp\"]\nclassifiers = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n]\n\n[tool.poetry.scripts]\nmcp-whisper = \"src.whisper_mcp_server:main\"\n```\n\n2. **Create LICENSE file** (MIT License)\n\n3. **Set up GitHub Actions** (.github/workflows/ci.yml):\n```yaml\nname: CI\non: [push, pull_request]\n\njobs:\n  test:\n    runs-on: macos-latest\n    strategy:\n      matrix:\n        python-version: [3.9, 3.10, 3.11]\n    \n    steps:\n    - uses: actions/checkout@v3\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n    - name: Install dependencies\n      run: |\n        brew install ffmpeg\n        pip install poetry\n        poetry install\n    - name: Run tests\n      run: poetry run pytest\n    - name: Check formatting\n      run: |\n        poetry run black --check .\n        poetry run isort --check .\n```\n\n4. **Create release process**:\n- Tag version in git: `git tag v1.0.0`\n- Build distribution: `poetry build`\n- Create GitHub release with:\n  - Release notes from CHANGELOG\n  - Built wheel and sdist files\n  - Installation instructions\n\n5. **Publish to PyPI**:\n```bash\n# Configure PyPI token\npoetry config pypi-token.pypi <your-token>\n\n# Publish\npoetry publish\n```\n\n6. **Update MCP server registry**:\n- Submit PR to official MCP registry\n- Include server description and capabilities\n- Add usage examples\n\n7. **Create demo materials**:\n- Record demo video showing:\n  - Installation process\n  - Single file transcription\n  - Batch processing\n  - Different output formats\n- Create blog post announcing release\n- Share on social media\n\n8. **Post-release checklist**:\n- Monitor GitHub issues\n- Set up discussion forum\n- Plan roadmap for v2.0 features:\n  - Speaker diarization\n  - Real-time transcription\n  - Cloud storage integration\n  - Web UI",
        "testStrategy": "- Test full installation on clean macOS system\n- Verify PyPI package installs correctly\n- Test GitHub Actions workflow\n- Ensure all documentation links work\n- Test demo examples\n- Get beta user feedback before public release",
        "status": "pending",
        "dependencies": [
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-06-15T20:25:09.429Z",
      "updated": "2025-07-14T22:28:13.633Z",
      "description": "Tasks for master context - Updated for FastMCP Python implementation"
    }
  }
}